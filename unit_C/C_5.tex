\Exercise[number={5}]
A variable \(x\) in \(\mathbf{R}^2\) is emitted by two possible sources,
\(w_1\), \(w_2\) of probability \(Pr(w_1)=p\) and \(Pr(w_2)=1-p\). 
We want to design a Bayes classifier that assigns the correct class to each
observed \(x\). If the \(p(x|w_i)\) both have a Gaussian distribution, with
mean \(\mu_i\) and equal variances \(I\sigma^2\), show that for each \(p\)
the decision regions are bounded by a line perpendicular to the segment
\(\mu_1-\mu_2\).

\Answer[number={5}]
The boundaries of a binary classifier can be studied by computing the
sign of \(z(x)\), with \(z\) being the exponent of the sigmoid function, in
particular the decision boundaries are given by \(z(x)=0\).
\begin{align*}
    z(x)
    &=\log{\frac{p(x|w_1)}{p(x|w_2)}} + \log{\frac{Pr(w_1)}{Pr(w_2)}}
    =\log{p(x|w_1)}-\log{p(x|w_2)}+\log{\frac{p}{1-p}}\\
    &=-\cancel{\log{\frac{2\pi}{2\pi}}}-\cancel{\frac{1}{2}\log{\frac{|\Sigma_1|}{|\Sigma_2|}}}-\frac{1}{2\sigma^2}\bigl[(x-\mu_1)^T(x-\mu_1)-(x-\mu_2)^T(x-\mu_2)\bigr]+\log{\frac{p}{1-p}}\\
    &=-\frac{1}{2\sigma^2}\bigl[(\mu_1^T\mu_1-\mu_2^T\mu_2)-2(\mu_1-\mu_2)^Tx\bigr]+\log{\frac{p}{1-p}}\\
    &=-\frac{1}{2\sigma^2}\bigl[(\mu_1-\mu_2)^T(\mu_1+\mu_2-2x)\bigr]+\log{\frac{p}{1-p}}\\
    &=\frac{1}{\sigma^2}\biggl[(\mu_1-\mu_2)^T(x-\frac{\mu_1-\mu_2}{2})\biggr]+\log{\frac{p}{1-p}}
\end{align*}
\begin{align*}
    z(x)=0 \Longleftrightarrow \frac{1}{\sigma^2}\biggl[(\mu_1-\mu_2)^T(x-\frac{\mu_1-\mu_2}{2})\biggr]+\log{\frac{p}{1-p}}=0
\end{align*}
Notice that the first part of the equation is derived from
\(\log{\frac{p(x|w_1)}{p(x|w_2)}}\) and it must be zero on the decision
boundary, as both the likelihoods must be equal to 0.5. Moreover, it represents
a scalar product between two vectors: the first one represents the line
connecting \(\mu_1\) and \(\mu_2\), the second one is the line connecting
a generic \(x\) to the middle point of the line connecting the two means.
However, due to the presence of the right-side term \(\log{\frac{p}{1-p}}\),
the intersection point with the \(\mu_1-\mu_2\) line is moved from
the middle point towards the class center having a lower probability \(Pr(w_i)\),
making the area on the plane accounting for that lower probability class smaller.
In the 3D space, the equation \(z(x)=(\mu_1-\mu_2)^T(x-\frac{\mu_1-\mu_2}{2})\)
represents a hyperplane (in this case a plane), with an offset
\(\log{\frac{p}{1-p}}\), affecting where the plane intersects the plane
generated by \(x_1\) and \(x_2\) components.
Hence, three cases are possible:
\begin{enumerate}
    \item \(Pr(w_1)=Pr(w_2)=0.5\Rightarrow\log{\frac{p}{1-p}}=0\):
    the scalar product between the aforementioned vectors is null, implying
    that the two vectors must be perpendicular.
    \item \(Pr(w_1)=p>Pr(w_2)=1-p\Rightarrow\log{\frac{p}{1-p}}>0\):
    the scalar product must once again be zero, however, the factor
    \(\log{\frac{p}{1-p}}\) implies a translation of the decision line
    toward the \(\mu_2\) mean. This is consistent with a translation
    toward the negative side of the sigmoid, since there is a bias
    toward the selection of class \(w_1\) (\(Pr(w_1)>0.5\)).
    \item \(Pr(w_1)=p<Pr(w_2)=1-p\Rightarrow\log{\frac{p}{1-p}}<0\):
    the scalar product must once again be zero, however, the factor
    \(\log{\frac{p}{1-p}}\) implies a translation of the decision line
    toward the \(\mu_1\) mean. This is consistent with a translation
    toward the positive side of the sigmoid, since there is a bias
    toward the selection of class \(w_2\) (\(Pr(w_2)>0.5\)).
  \end{enumerate}
  A proof of the fact that all the three decision lines display the same
  slope (thus perpendicular to the \(\mu_1-\mu_2\) line) is given below:
  \begin{enumerate}
    \item \(Pr(w_1)=Pr(w_2)=0.5\Rightarrow\log{\frac{p}{1-p}}=0\):
    \begin{align*}
        z(x)=0
        &\Rightarrow
        (\mu_1-\mu_2)^T(x-\frac{\mu_1-\mu_2}{2})=0\\
        &\Rightarrow
        \begin{bmatrix}
            \mu_{1_{x_1}}-\mu_{2_{x_1}} && \mu_{1_{x_2}}-\mu_{2_{x_2}}
        \end{bmatrix}
        \begin{bmatrix}
            x_1-\frac{\mu_{1_{x_1}}-\mu_{2_{x_1}}}{2} \\ x_2-\frac{\mu_{1_{x_2}}-\mu_{2_{x_2}}}{2}
        \end{bmatrix}
        =0\\
        &\Rightarrow
        \begin{bmatrix}
            c_1 && c_2
        \end{bmatrix}
        \begin{bmatrix}
            x_1-\frac{c_1}{2} \\ x_2-\frac{c_2}{2}
        \end{bmatrix}
        =0\\
        &\Rightarrow
        c_1x_1-\frac{c_1^2}{2}+c_2x_2-\frac{c_2^2}{2}=0
        \Rightarrow
        x_2 = -\frac{c_1}{c_2}x_1+\frac{c_1^2+c_2^2}{2c_2}
    \end{align*}
    Therefore, the line slope is \(m=-\frac{c_1}{c_2}\).
    \item \(Pr(w_1)=p>Pr(w_2)=1-p\Rightarrow\log{\frac{p}{1-p}}>0\):
    \begin{align*}
        z(x)=0
        &\Rightarrow
        (\mu_1-\mu_2)^T(x-\frac{\mu_1-\mu_2}{2})+\log{\frac{p}{1-p}}=0\\
        &\Rightarrow
        \begin{bmatrix}
            \mu_{1_{x_1}}-\mu_{2_{x_1}} && \mu_{1_{x_2}}-\mu_{2_{x_2}}
        \end{bmatrix}
        \begin{bmatrix}
            x_1-\frac{\mu_{1_{x_1}}-\mu_{2_{x_1}}}{2} \\ x_2-\frac{\mu_{1_{x_2}}-\mu_{2_{x_2}}}{2}
        \end{bmatrix}
        =-\log{\frac{p}{1-p}}\\
        &\Rightarrow
        \begin{bmatrix}
            c_1 && c_2
        \end{bmatrix}
        \begin{bmatrix}
            x_1-\frac{c_1}{2} \\ x_2-\frac{c_2}{2}
        \end{bmatrix}
        =-\log{\frac{p}{1-p}}\\
        &\Rightarrow
        c_1x_1-\frac{c_1^2}{2}+c_2x_2-\frac{c_2^2}{2}=-\log{\frac{p}{1-p}}
        \Rightarrow
        x_2 = -\frac{c_1}{c_2}x_1+\frac{c_1^2+c_2^2}{2c_2}-\frac{1}{c_2}\log{\frac{p}{1-p}}
    \end{align*}
    Notice the line slope is still \(m=-\frac{c_1}{c_2}\), but the
    y-intercept has an additional term, shifting the
    decision boundary toward \(\mu_2\), the centre of class \(w_2\).
    \item \(Pr(w_1)=p<Pr(w_2)=1-p\Rightarrow\log{\frac{p}{1-p}}<0\):
    \begin{align*}
        z(x)=0
        &\Rightarrow
        (\mu_1-\mu_2)^T(x-\frac{\mu_1-\mu_2}{2})+\log{\frac{p}{1-p}}=0\\
        &\Rightarrow
        \begin{bmatrix}
            \mu_{1_{x_1}}-\mu_{2_{x_1}} && \mu_{1_{x_2}}-\mu_{2_{x_2}}
        \end{bmatrix}
        \begin{bmatrix}
            x_1-\frac{\mu_{1_{x_1}}-\mu_{2_{x_1}}}{2} \\ x_2-\frac{\mu_{1_{x_2}}-\mu_{2_{x_2}}}{2}
        \end{bmatrix}
        =-\log{\frac{p}{1-p}}\\
        &\Rightarrow
        \begin{bmatrix}
            c_1 && c_2
        \end{bmatrix}
        \begin{bmatrix}
            x_1-\frac{c_1}{2} \\ x_2-\frac{c_2}{2}
        \end{bmatrix}
        =-\log{\frac{p}{1-p}}\\
        &\Rightarrow
        c_1x_1-\frac{c_1^2}{2}+c_2x_2-\frac{c_2^2}{2}=-\log{\frac{p}{1-p}}
        \Rightarrow
        x_2 = -\frac{c_1}{c_2}x_1+\frac{c_1^2+c_2^2}{2c_2}-\frac{1}{c_2}\log{\frac{p}{1-p}}
    \end{align*}
    Also here the slope is \(m=-\frac{c_1}{c_2}\), but the
    y-intercept has an additional term of opposite sign w.r.t. the
    previous case, thus the decision boundary shifts toward
    \(\mu_1\), the centre of class \(w_1\).
  \end{enumerate}